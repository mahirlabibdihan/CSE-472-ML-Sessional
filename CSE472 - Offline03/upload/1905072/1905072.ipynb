{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.629809Z","iopub.status.busy":"2024-10-16T17:59:37.629021Z","iopub.status.idle":"2024-10-16T17:59:37.634352Z","shell.execute_reply":"2024-10-16T17:59:37.633325Z","shell.execute_reply.started":"2024-10-16T17:59:37.629766Z"},"trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.658722Z","iopub.status.busy":"2024-10-16T17:59:37.657679Z","iopub.status.idle":"2024-10-16T17:59:37.663372Z","shell.execute_reply":"2024-10-16T17:59:37.662037Z","shell.execute_reply.started":"2024-10-16T17:59:37.658674Z"},"trusted":true},"outputs":[],"source":["SAVE_OUTPUT = True"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.666097Z","iopub.status.busy":"2024-10-16T17:59:37.665622Z","iopub.status.idle":"2024-10-16T17:59:37.676069Z","shell.execute_reply":"2024-10-16T17:59:37.674771Z","shell.execute_reply.started":"2024-10-16T17:59:37.666047Z"},"trusted":true},"outputs":[],"source":["class Layer:\n","    def __init__(self):\n","        self.input = None\n","        self.output = None\n","\n","    # reinitialize the layer\n","    def reset(self):\n","        self.input = None\n","        self.output = None\n","        \n","    def restore(self, input, output):\n","        self.input = input\n","        self.output = output\n","        \n","    def save(self):\n","        return (self.input, self.output)\n","        \n","    def forward(self, input, training=True):\n","        pass\n","\n","    def backward(self, d_output, learning_rate):\n","        pass"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.688643Z","iopub.status.busy":"2024-10-16T17:59:37.688255Z","iopub.status.idle":"2024-10-16T17:59:37.698619Z","shell.execute_reply":"2024-10-16T17:59:37.697409Z","shell.execute_reply.started":"2024-10-16T17:59:37.688604Z"},"trusted":true},"outputs":[],"source":["class Flatten(Layer):\n","    def __init__(self, input_shape):\n","        self.input_shape = input_shape\n","        \n","    def reset(self):\n","        pass\n","    \n","    def save(self):\n","        return {}\n","    \n","    def restore(self):\n","        pass\n","    \n","    def forward(self, inputs, training=True):\n","        if len(inputs.shape) <= 1:\n","           raise ValueError(f\"Flatten layer requires input with more than 1 dimension. Received shape: {inputs.shape}.\")\n","\n","        if inputs.shape[1:] != self.input_shape:\n","            raise ValueError(f\"Input shape {inputs.shape[1:]} does not match expected shape {self.input_shape}.\")\n","        \n","        return inputs.reshape(inputs.shape[0], -1)\n","    \n","    def backward(self, d_output, optimizer=None):\n","        batch_size = d_output.shape[0]\n","        return d_output.reshape(batch_size, *self.input_shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Optimization: Adaptive Moment Estimation (Adam)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.701943Z","iopub.status.busy":"2024-10-16T17:59:37.701153Z","iopub.status.idle":"2024-10-16T17:59:37.709657Z","shell.execute_reply":"2024-10-16T17:59:37.708524Z","shell.execute_reply.started":"2024-10-16T17:59:37.701888Z"},"trusted":true},"outputs":[],"source":["class Optimizer:\n","    def __init__(self, params):\n","        pass\n","    \n","    def update(self, layer):\n","        pass"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.711975Z","iopub.status.busy":"2024-10-16T17:59:37.711019Z","iopub.status.idle":"2024-10-16T17:59:37.721764Z","shell.execute_reply":"2024-10-16T17:59:37.720529Z","shell.execute_reply.started":"2024-10-16T17:59:37.711933Z"},"trusted":true},"outputs":[],"source":["class Adam(Optimizer):\n","    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n","        self.lr = learning_rate\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.eps = eps\n","            \n","    def update(self, param, gradient, m, v, t):\n","        m = self.beta1 * m + (1 - self.beta1) * gradient\n","        v = self.beta2 * v + (1 - self.beta2) * np.power(gradient, 2)\n","        m_hat = m / (1 - np.power(self.beta1, t))\n","        v_hat = v / (1 - np.power(self.beta2, t))\n","        param = param - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n","        return param, m, v"]},{"cell_type":"markdown","metadata":{},"source":["# Dense Layer: A fully connected layer, defined by the dimensions of its input and output."]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.723689Z","iopub.status.busy":"2024-10-16T17:59:37.723254Z","iopub.status.idle":"2024-10-16T17:59:37.740146Z","shell.execute_reply":"2024-10-16T17:59:37.738907Z","shell.execute_reply.started":"2024-10-16T17:59:37.723643Z"},"trusted":true},"outputs":[],"source":["class Dense(Layer):\n","    def __init__(self, input_size, output_size, activation=None):\n","        # Initialize weights and biases with float32\n","        self.weights = np.random.randn(input_size, output_size).astype(np.float32) * 0.01\n","        self.bias = np.zeros((1, output_size), dtype=np.float32)\n","        self.activation = activation\n","        \n","        # For optimizer\n","        self.m_w = np.zeros((input_size, output_size), dtype=np.float32)  # momentum for weights\n","        self.v_w = np.zeros((input_size, output_size), dtype=np.float32)  # velocity for weights\n","        self.m_b = np.zeros((1, output_size), dtype=np.float32)  # momentum for bias\n","        self.v_b = np.zeros((1, output_size), dtype=np.float32)  # velocity for bias\n","        self.t = 0 # step counter for bias correction\n","\n","    def reset(self):\n","        self.weights = np.random.randn(self.weights.shape[0], self.weights.shape[1]).astype(np.float32) * 0.01\n","        self.bias = np.zeros((1, self.bias.shape[1]), dtype=np.float32)\n","        \n","        self.m_w = np.zeros_like(self.m_w)\n","        self.v_w = np.zeros_like(self.v_w)\n","        self.m_b = np.zeros_like(self.m_b)\n","        self.v_b = np.zeros_like(self.v_b)\n","        self.t = 0\n","        \n","    def restore(self, weights, bias, m_w, v_w, m_b, v_b, t):\n","        self.weights = weights.astype(np.float32)\n","        self.bias = bias.astype(np.float32)\n","        \n","        self.m_w = m_w.astype(np.float32)\n","        self.v_w = v_w.astype(np.float32)\n","        self.m_b = m_b.astype(np.float32)\n","        self.v_b = v_b.astype(np.float32)\n","        self.t = t\n","        \n","    def save(self):\n","        return {\n","            'weights': self.weights.astype(np.float32),\n","            'bias': self.bias.astype(np.float32),\n","            'm_w': self.m_w.astype(np.float32),\n","            'v_w': self.v_w.astype(np.float32),\n","            'm_b': self.m_b.astype(np.float32),\n","            'v_b': self.v_b.astype(np.float32),\n","            't': self.t\n","        }\n","        \n","    def forward(self, X, training=True):\n","        self.input = X  # Save input for backpropagation\n","        self.output = np.dot(X, self.weights) + self.bias\n","            # if self.activation == 'relu':\n","            #     self.output = np.maximum(0, self.output)\n","                \n","        return self.output\n","\n","    def backward(self, d_output, optimizer):\n","        # Compute gradients for weights and bias\n","        # Output, O = XW + b \n","        # d/dW (O) = X\n","        # d/dW (L) = d/dW (O) * d/dO (L) = X * d/dO (L) = X * d_output\n","        d_weights = np.dot(self.input.T, d_output)\n","        d_bias = np.sum(d_output, axis=0, keepdims=True)\n","\n","        self.t += 1\n","        self.weights, self.m_w, self.v_w = optimizer.update(self.weights, d_weights, self.m_w, self.v_w, self.t)\n","        self.bias, self.m_b, self.v_b = optimizer.update(self.bias, d_bias, self.m_b, self.v_b, self.t)\n","\n","        # Compute gradient with respect to the input\n","        d_input = np.dot(d_output, self.weights.T)\n","        return d_input"]},{"cell_type":"markdown","metadata":{},"source":["# Batch Normalization"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.743523Z","iopub.status.busy":"2024-10-16T17:59:37.743119Z","iopub.status.idle":"2024-10-16T17:59:37.759960Z","shell.execute_reply":"2024-10-16T17:59:37.758786Z","shell.execute_reply.started":"2024-10-16T17:59:37.743481Z"},"trusted":true},"outputs":[],"source":["class BatchNormalization(Layer):\n","    def __init__(self, input_size, learning_rate=0.001, eps=1e-5, momentum=0.1):\n","        self.eps = eps\n","        self.momentum = momentum\n","        self.gamma =  np.ones((1, input_size), dtype=np.float32)\n","        self.beta = np.zeros((1, input_size), dtype=np.float32)\n","        self.moving_mean = np.zeros((1, input_size), dtype=np.float32)\n","        self.moving_var = np.ones((1, input_size), dtype=np.float32)\n","        self.lr = learning_rate\n","        \n","    def reset(self):\n","        self.gamma = np.ones_like(self.gamma, dtype=np.float32)\n","        self.beta = np.zeros_like(self.beta, dtype=np.float32)\n","        self.moving_mean = np.zeros_like(self.moving_mean, dtype=np.float32)\n","        self.moving_var = np.ones_like(self.moving_var, dtype=np.float32)\n","        \n","    def restore(self, gamma, beta, moving_mean, moving_var):\n","        self.gamma = gamma.astype(np.float32)\n","        self.beta = beta.astype(np.float32)\n","        self.moving_mean = moving_mean.astype(np.float32)\n","        self.moving_var = moving_var.astype(np.float32)\n","        \n","    def save(self): \n","        return {\n","            'gamma': self.gamma.astype(np.float32),\n","            'beta': self.beta.astype(np.float32),\n","            'moving_mean': self.moving_mean.astype(np.float32), \n","            'moving_var': self.moving_var.astype(np.float32)\n","        }\n","\n","    def forward(self, X, training=True):\n","        self.input = X\n","        if training:\n","            # When using a fully connected layer, calculate the mean and\n","            # variance on the feature dimension\n","            batch_mean = np.mean(X, axis=0, keepdims=True).astype(np.float32)\n","            batch_var = np.var(X, axis=0, keepdims=True).astype(np.float32)\n","            \n","            # In training mode, the current mean and variance are used\n","            self.X_centered = X - batch_mean\n","            self.stddev_inv = 1.0 / np.sqrt(batch_var + self.eps)\n","            self.X_hat = self.X_centered * self.stddev_inv\n","\n","            # Update the mean and variance using moving average - Needed for inference\n","            self.moving_mean = (1.0 - self.momentum) * self.moving_mean + self.momentum * batch_mean\n","            self.moving_var = (1.0 - self.momentum) * self.moving_var + self.momentum * batch_var\n","        else:\n","            self.X_hat = (X - self.moving_mean) / np.sqrt(self.moving_var + self.eps)\n","\n","        self.output = self.gamma * self.X_hat + self.beta\n","        return self.output.astype(np.float32)\n","\n","    def backward(self, d_output, optimizer=None):\n","        N, D = d_output.shape\n","\n","        self.d_gamma = np.sum(d_output * self.X_hat, axis=0)\n","        self.d_beta = np.sum(d_output, axis=0)\n","\n","        d_X_norm = d_output * self.gamma\n","        d_var = np.sum(d_X_norm * self.X_centered * -0.5 * np.power(self.stddev_inv,3), axis=0)\n","        d_mean = np.sum(d_X_norm * -self.stddev_inv, axis=0) + d_var * np.mean(-2. * self.X_centered, axis=0)\n","\n","        d_input = (d_X_norm * self.stddev_inv) + (d_var * 2 * self.X_centered / N) + (d_mean / N)\n","\n","        # Update parameters\n","        self.gamma -= self.lr * self.d_gamma\n","        self.beta -= self.lr * self.d_beta\n","\n","        return d_input"]},{"cell_type":"markdown","metadata":{},"source":["# Activation: ReLU"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.761563Z","iopub.status.busy":"2024-10-16T17:59:37.761227Z","iopub.status.idle":"2024-10-16T17:59:37.778966Z","shell.execute_reply":"2024-10-16T17:59:37.777983Z","shell.execute_reply.started":"2024-10-16T17:59:37.761530Z"},"trusted":true},"outputs":[],"source":["class ReLU(Layer):\n","    def __init__(self):\n","        pass\n","    \n","    def reset(self):\n","        pass\n","    \n","    def restore(self):\n","        pass\n","    \n","    def save(self):\n","        return {}\n","        \n","    def forward(self, X, training=True):\n","        self.input = X\n","        return np.maximum(0, self.input)\n","\n","    def backward(self, d_output, optimizer=None):\n","        # For positive values of input, the gradient is 1.\n","        # For non-positive values (0 or negative) of input, the gradient is 0.\n","        return d_output * (self.input > 0)"]},{"cell_type":"markdown","metadata":{},"source":["# Regularization: Dropout"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.780583Z","iopub.status.busy":"2024-10-16T17:59:37.780249Z","iopub.status.idle":"2024-10-16T17:59:37.790574Z","shell.execute_reply":"2024-10-16T17:59:37.789229Z","shell.execute_reply.started":"2024-10-16T17:59:37.780545Z"},"trusted":true},"outputs":[],"source":["class Dropout(Layer):\n","    def __init__(self, dropout_rate):\n","        assert 0 <= dropout_rate <= 1\n","        self.dropout_rate = dropout_rate\n","        \n","    def reset(self):\n","        pass\n","    \n","    def restore(self):\n","        pass\n","    \n","    def save(self):\n","        return {}\n","\n","    def forward(self, X, training=True):\n","        self.input = X\n","        if training: # Only apply dropout during training!!\n","            if self.dropout_rate == 1.0:\n","                return np.zeros_like(X)\n","                \n","            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=X.shape) \n","            \n","            # Need to normalize the values to keep the expected value the same\n","            return (X * self.mask) / (1 - self.dropout_rate)\n","        else:\n","            return X\n","\n","    def backward(self, d_output, optimizer=None):\n","        return d_output * self.mask"]},{"cell_type":"markdown","metadata":{},"source":["# Regression: Softmax for Multi-class Classification"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.792399Z","iopub.status.busy":"2024-10-16T17:59:37.792027Z","iopub.status.idle":"2024-10-16T17:59:37.806870Z","shell.execute_reply":"2024-10-16T17:59:37.805653Z","shell.execute_reply.started":"2024-10-16T17:59:37.792359Z"},"trusted":true},"outputs":[],"source":["class Softmax(Layer):\n","    def __init__(self):\n","        pass\n","    \n","    def reset(self):\n","        pass\n","    \n","    def restore(self):\n","        pass\n","    \n","    def save(self):\n","        return {}\n","    \n","    def forward(self, X, training=True):\n","        self.input = X\n","        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n","        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n","        return self.output\n","\n","    def backward(self, d_output, optimizer=None):\n","        d_input = self.output * (d_output - np.sum(self.output * d_output, axis=-1, keepdims=True))\n","        return d_input\n","    \n","    # def backward(self, d_output, optimizer=None):\n","    #     d_input = d_output\n","    #     return d_input\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Cross-Entropy Loss"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.808920Z","iopub.status.busy":"2024-10-16T17:59:37.808430Z","iopub.status.idle":"2024-10-16T17:59:37.817695Z","shell.execute_reply":"2024-10-16T17:59:37.816597Z","shell.execute_reply.started":"2024-10-16T17:59:37.808876Z"},"trusted":true},"outputs":[],"source":["class Loss:\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, y_true, y_pred):\n","        pass\n","    \n","    def backward(self, y_true, y_pred):\n","        pass"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.819852Z","iopub.status.busy":"2024-10-16T17:59:37.819346Z","iopub.status.idle":"2024-10-16T17:59:37.831508Z","shell.execute_reply":"2024-10-16T17:59:37.830164Z","shell.execute_reply.started":"2024-10-16T17:59:37.819778Z"},"trusted":true},"outputs":[],"source":["class CategoricalCrossEntropyLoss(Loss):\n","    def forward(self, y_pred, y_true):\n","        # Clip predictions to avoid log(0) or log(negative)\n","        epsilon = 1e-15\n","        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n","\n","        n_samples = len(y_true)\n","        \n","        log_likelihood = y_true * np.log(y_pred)\n","\n","        # Compute the loss\n","        loss = -(1 / n_samples) * np.sum(log_likelihood)\n","        \n","        return loss\n","\n","    def backward(self, y_pred, y_true):\n","        epsilon = 1e-15    \n","        n_samples = len(y_true)\n","        \n","        y_pred = np.maximum(y_pred, epsilon)\n","        d_input = -y_true / y_pred\n","        \n","        return (1 / n_samples) * d_input\n","\n","    # https://stackoverflow.com/a/76532286: The usual workaround to prevent computing the Jacobian of the softmax, is to simply take derivatives of the Loss function, with respect to the inputs passed to softmax, instead of the outputs passed by softmax. \n","    # def backward(self, y_pred, y_true):\n","    #     d_input_of_softmax = y_pred - y_true\n","    #     return d_input_of_softmax"]},{"cell_type":"markdown","metadata":{},"source":["# Neural Network"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T21:04:33.992291Z","iopub.status.busy":"2024-10-16T21:04:33.991031Z","iopub.status.idle":"2024-10-16T21:04:34.024671Z","shell.execute_reply":"2024-10-16T21:04:34.023262Z","shell.execute_reply.started":"2024-10-16T21:04:33.992238Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import os\n","import shutil\n","import pickle\n","\n","class NeuralNetwork:\n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    def forward(self, X, training=True):\n","        \"\"\"Perform the forward pass through all layers.\"\"\"\n","        for layer in self.layers:\n","            X = layer.forward(X, training)\n","\n","        return X\n","\n","    def backward(self, d_output):\n","        \"\"\"Perform the backward pass through all layers.\"\"\"\n","        for layer in reversed(self.layers):\n","            d_output = layer.backward(d_output, self.optimizer)\n","\n","    def save(self, file_path):\n","        state = {'layers': [layer.save() for layer in self.layers]}\n","        \n","        with open(file_path, 'wb') as f:\n","            pickle.dump(state, f)\n","\n","    def restore(self, file_path):\n","        with open(file_path, 'rb') as f:\n","            state = pickle.load(f)\n","        for layer, layer_state in zip(self.layers, state['layers']):\n","            # print(layer_state)\n","            layer.restore(**layer_state)\n","            \n","    def compile(self, optimizer, loss):\n","        self.optimizer = optimizer\n","        self.loss = loss\n","            \n","    def train(self, X_train, y_train, model_id, lr_id, X_val = None, y_val = None, epochs = 25, batch_size = 32, DEBUG=False):\n","        \"\"\"Train the network using mini-batch gradient descent.\"\"\"\n","        n_samples = X_train.shape[0]\n","        train_losses = []\n","        val_losses = []\n","        train_acc = []\n","        val_acc = []\n","        val_f1_scores = []\n","\n","        if SAVE_OUTPUT and DEBUG:\n","            output_dir = f'report/logs/{model_id}/{lr_id}'\n","            \n","            # Clear the folder if it exists\n","            if os.path.exists(output_dir):\n","                shutil.rmtree(output_dir)\n","                \n","            os.makedirs(output_dir, exist_ok=True)\n","            \n","        for epoch in range(epochs):\n","            # Shuffle data\n","            indices = np.arange(n_samples)\n","            np.random.shuffle(indices)\n","            \n","            X_train = X_train[indices]\n","            y_train = y_train[indices]\n","\n","            epoch_loss = 0\n","            correct_predictions = 0\n","\n","            # Mini-batch gradient descent\n","            for start_idx in range(0, n_samples, batch_size):\n","                end_idx = start_idx + batch_size\n","                X_batch = X_train[start_idx:end_idx]\n","                y_batch = y_train[start_idx:end_idx]\n","\n","                # Forward pass\n","                predictions = self.forward(X_batch)\n","                # Compute the loss\n","                epoch_loss +=  self.loss.forward(predictions, y_batch)\n","\n","                # Backward pass (backpropagation)\n","                d_output = self.loss.backward(predictions, y_batch)\n","                self.backward(d_output)\n","                \n","                # Calculate training accuracy\n","                pred_labels = np.argmax(predictions, axis=1)\n","                true_labels = np.argmax(y_batch, axis=1)\n","                correct_predictions += np.sum(pred_labels == true_labels)\n","\n","            # Average loss over the epoch\n","            avg_loss = epoch_loss / (n_samples // batch_size)\n","            train_losses.append(avg_loss)\n","            \n","            train_accuracy = correct_predictions / n_samples\n","            train_acc.append(train_accuracy)\n","\n","            \n","            # Add validation loss \n","            if X_val is not None and y_val is not None:\n","                val_predictions = self.forward(X_val)\n","                val_loss = self.loss.forward(val_predictions, y_val)\n","                val_losses.append(val_loss)\n","\n","                # Save in log file\n","                if SAVE_OUTPUT and DEBUG:\n","                    with open(os.path.join(output_dir, 'log.txt'), 'a') as f:\n","                        f.write(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss}, Validation Loss: {val_loss}\\n\")\n","                        \n","                if epoch % 5 == 0:\n","                    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss}, Validation Loss: {val_loss}\")\n","\n","                    \n","                        \n","                # Calculate validation accuracy\n","                val_pred_labels = np.argmax(val_predictions, axis=1)\n","                val_true_labels = np.argmax(y_val, axis=1)\n","                val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","                val_acc.append(val_accuracy)\n","\n","                f1 = f1_score(val_true_labels,\n","                              val_pred_labels, average='macro')\n","                val_f1_scores.append(f1)\n","        \n","        if X_val is not None and y_val is not None:\n","            if SAVE_OUTPUT and DEBUG:\n","                output_dir = f'report/images/{model_id}/{lr_id}'\n","                os.makedirs(output_dir, exist_ok=True)\n","            # Plotting the training and validation loss\n","            plt.figure(figsize=(10, 5))\n","            plt.plot(train_losses, label='Training Loss')\n","            plt.plot(val_losses, label='Validation Loss')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Loss')\n","            plt.title('Training and Validation Loss')\n","            plt.legend()\n","\n","            if SAVE_OUTPUT and DEBUG:\n","                plt.savefig(os.path.join(output_dir, 'training_validation_loss.png'))\n","            plt.show()\n","\n","            # Plotting the training and validation acc\n","            plt.figure(figsize=(10, 5))\n","            plt.plot(train_acc, label='Training Accuracy')\n","            plt.plot(val_acc, label='Validation Accuracy')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Accuracy')\n","            plt.title('Training and Validation Accuracy')\n","            plt.legend()\n","\n","            if SAVE_OUTPUT and DEBUG:\n","                plt.savefig(os.path.join(output_dir, 'training_validation_accuracy.png'))\n","            plt.show()\n","\n","            # Plotting the validation F1 score\n","            plt.figure(figsize=(10, 5))\n","            plt.plot(val_f1_scores, label='Validation F1 Score')\n","            plt.xlabel('Epochs')\n","            plt.ylabel('F1 Score')\n","            plt.title('Validation F1 Score')\n","            plt.legend()\n","\n","            if SAVE_OUTPUT and DEBUG:\n","                plt.savefig(os.path.join(output_dir, 'validation_f1_score.png'))\n","            plt.show()\n","\n","            # Plot confusion matrix\n","            # val_predictions = self.forward(X_val)\n","            val_pred_labels = np.argmax(val_predictions, axis=1)\n","            val_true_labels = np.argmax(y_val, axis=1)\n","            conf_matrix = confusion_matrix(val_true_labels, val_pred_labels)\n","            plt.figure(figsize=(10, 7))\n","            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n","            plt.xlabel('Predicted Labels')\n","            plt.ylabel('True Labels')\n","            plt.title('Confusion Matrix')\n","\n","            if SAVE_OUTPUT and DEBUG:\n","                plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n","            plt.show()\n","\n","            return val_f1_scores[-1]"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.867255Z","iopub.status.busy":"2024-10-16T17:59:37.866885Z","iopub.status.idle":"2024-10-16T17:59:37.881431Z","shell.execute_reply":"2024-10-16T17:59:37.880169Z","shell.execute_reply.started":"2024-10-16T17:59:37.867217Z"},"trusted":true},"outputs":[],"source":["def one_hot_encode(y, num_classes=10):\n","    return np.eye(num_classes)[y]"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:37.883534Z","iopub.status.busy":"2024-10-16T17:59:37.883040Z","iopub.status.idle":"2024-10-16T17:59:44.605320Z","shell.execute_reply":"2024-10-16T17:59:44.604255Z","shell.execute_reply.started":"2024-10-16T17:59:37.883492Z"},"trusted":true},"outputs":[],"source":["\n","from torchvision import datasets, transforms\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","np.random.seed(1905072)\n","transform = transforms.ToTensor()\n","train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","\n","# Initialize empty lists to store X_train and y_train\n","X = []\n","y = []\n","\n","# Iterate over the dataset\n","for img, label in train_dataset:\n","    X.append(img.numpy().squeeze(0))  # Convert the tensor to NumPy array\n","    y.append(label)        # Labels are already integers, so just append them\n","\n","# Convert the list of arrays to NumPy arrays\n","X = np.array(X)\n","y = np.array(y)\n","\n","# Apply one-hot encoding to y_train\n","y_one_hot = one_hot_encode(y)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.15, random_state=42)\n","\n","# Print the shapes of the datasets\n","print(\"Training set shape:\", X_train.shape, y_train.shape)\n","print(\"Validation set shape:\", X_val.shape, y_val.shape)\n","\n","learning_rates = [5e-3,1e-3,5e-4,1e-4]\n","\n","best_model = None\n","best_f1 = 0.0\n","best_lr = 0.0\n","best_idx = -1\n","\n","def get_model(model_id, lr):\n","    if model_id == 0:\n","        return [\n","            Flatten(input_shape=(28,28)), # Dropout(0.2), \n","            Dense(784, 256), BatchNormalization(256, lr), ReLU(), Dropout(0.2), \n","            Dense(256, 10), Softmax()\n","        ]\n","    elif model_id == 1: \n","        return [\n","            Flatten(input_shape=(28,28)), # Dropout(0.2), \n","            Dense(784, 180), BatchNormalization(180, lr), ReLU(), Dropout(0.2), \n","            Dense(180, 80), BatchNormalization(80), ReLU(), Dropout(0.2),\n","            Dense(80, 10), Softmax()\n","        ]\n","    elif model_id == 2:\n","        return [\n","            Flatten(input_shape=(28,28)), # Dropout(0.2), \n","            Dense(784, 150), BatchNormalization(150, lr), ReLU(), Dropout(0.2),\n","            Dense(150, 100), BatchNormalization(100, lr), ReLU(), Dropout(0.2),\n","            Dense(100, 50), BatchNormalization(50, lr), ReLU(), Dropout(0.2),\n","            Dense(50, 10), Softmax()\n","        ]\n","        \n","        \n","# Best Model\n","best_idx = 0\n","best_lr = 2"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T17:59:44.606988Z","iopub.status.busy":"2024-10-16T17:59:44.606649Z","iopub.status.idle":"2024-10-16T20:42:15.202849Z","shell.execute_reply":"2024-10-16T20:42:15.201109Z","shell.execute_reply.started":"2024-10-16T17:59:44.606954Z"},"trusted":true},"outputs":[],"source":["# for j in range(len(learning_rates) - 1, -1, -1):\n","for j in range(0,len(learning_rates)):\n","    learning_rate = learning_rates[j]\n","    optimizer = Adam(learning_rate=learning_rate)\n","    loss = CategoricalCrossEntropyLoss()\n","    networks = [\n","        get_model(0, learning_rate), \n","        get_model(1, learning_rate),\n","        get_model(2, learning_rate),\n","    ]\n","    \n","    # for i in range(len(networks) - 1, -1, -1):\n","    for i in range(0,len(networks)):\n","        for layer in networks[i]:\n","            layer.reset()\n","        \n","        print(\"Training Model:\", i, \"Learning Rate:\", learning_rate)\n","        \n","        layers = networks[i]\n","        nn = NeuralNetwork(layers)\n","        nn.compile(optimizer, loss)\n","        f1 = nn.train(X_train, y_train, X_val = X_val, y_val = y_val, epochs=25, batch_size=32, model_id=i, lr_id=j, DEBUG=True)\n","\n","        print(\"Model:\", i, \"Learning Rate:\", learning_rate, \"F1 Score:\", f1)\n","\n","        if SAVE_OUTPUT:\n","            # Save F1 score\n","            output_dir = f'report/results/{i}/{j}'\n","            os.makedirs(output_dir, exist_ok=True)\n","            with open(os.path.join(output_dir, \"f1_scores.log\"), 'w') as file:\n","                file.write(f\"{f1}\")\n","       \n","            # Save the model\n","            output_dir = f'models/{i}/{j}'\n","            os.makedirs(output_dir, exist_ok=True)\n","            with open(os.path.join(output_dir, 'trained_model.pkl'), 'wb') as f:\n","                pickle.dump(nn, f)\n","\n","            # Save the model\n","            nn.save(os.path.join(output_dir, 'model_params.pkl'))\n","\n","        if f1 >= best_f1:\n","            best_f1 = f1\n","            best_lr = j\n","            best_idx = i\n","            \n","print(\"Best F1 Score:\", best_f1) \n","print(\"Best Model:\", best_idx)\n","print(\"Best Learning Rate:\", learning_rates[best_lr])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Save the Best Model"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["model = NeuralNetwork(get_model(best_idx, learning_rates[best_lr]))\n","model.restore(f'models/{best_idx}/{best_lr}/model_params.pkl')\n","model.save('model_1905072.pickle')"]},{"cell_type":"markdown","metadata":{},"source":["# Test"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-10-16T21:12:07.103241Z","iopub.status.busy":"2024-10-16T21:12:07.102249Z","iopub.status.idle":"2024-10-16T21:14:17.829288Z","shell.execute_reply":"2024-10-16T21:14:17.828074Z","shell.execute_reply.started":"2024-10-16T21:12:07.103183Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 89.240%\n","F1 Score: 0.8919878734738692\n"]}],"source":["import pickle\n","import numpy as np\n","from torchvision import datasets, transforms\n","\n","# For reproducibility - Comment if retrains\n","\n","\n","model = NeuralNetwork(get_model(best_idx, learning_rates[best_lr]))\n","model.restore(f'model_1905072.pickle')\n","\n","# Transform to convert PIL image to tensor\n","transform = transforms.ToTensor()\n","\n","# Assuming you will be given a dataset for testing\n","test_dataset = datasets.FashionMNIST(\n","    root='./data', train=False, download=True, transform=transform)\n","\n","# Get test images and labels (this is provided, and you should load it appropriately)\n","X_test = []\n","y_test = []\n","\n","for img, label in test_dataset:\n","    X_test.append(img.numpy().squeeze(0))\n","    y_test.append(label)\n","\n","# Convert the list of arrays to NumPy arrays\n","X_test = np.array(X_test)\n","y_test = np.array(y_test)\n","\n","# Flatten the input if it's not already flattened\n","# if len(X_test.shape) > 2:\n","#     X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","# Fit to validation data\n","optimizer = Adam(learning_rate=learning_rates[best_lr])\n","loss = CategoricalCrossEntropyLoss()\n","\n","# Forward pass through the trained model to get predictions\n","predictions = model.forward(X_test, training=False)\n","\n","# Find the predicted class for each sample (assuming the last layer is softmax)\n","predicted_labels = np.argmax(predictions, axis=1)\n","\n","# Output the predictions\n","# print(predicted_labels)\n","\n","# If you need to compute the accuracy against ground truth\n","accuracy = np.mean(predicted_labels == y_test)\n","print(f\"Test Accuracy: {accuracy * 100:.3f}%\")\n","\n","# Find f1 macro\n","from sklearn.metrics import f1_score\n","f1 = f1_score(y_test, predicted_labels, average='macro')\n","print(f\"F1 Score: {f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","import numpy as np\n","from torchvision import datasets, transforms\n","\n","# For reproducibility - Comment if retrains\n","# best_idx = 0\n","# best_lr = 2\n","\n","# Iterate for best_idx and best_lr\n","\n","# for best_idx in range(3):\n","#     for best_lr in range(4):\n","#         model = NeuralNetwork(get_model(best_idx, learning_rates[best_lr]))\n","#         model.restore(f'models/{best_idx}/{best_lr}/model_params.pkl')\n","\n","#         # Transform to convert PIL image to tensor\n","#         transform = transforms.ToTensor()\n","\n","#         # Assuming you will be given a dataset for testing\n","#         test_dataset = datasets.FashionMNIST(\n","#             root='./data', train=False, download=True, transform=transform)\n","\n","#         # Get test images and labels (this is provided, and you should load it appropriately)\n","#         X_test = []\n","#         y_test = []\n","\n","#         for img, label in test_dataset:\n","#             X_test.append(img.numpy().squeeze(0))\n","#             y_test.append(label)\n","\n","#         # Convert the list of arrays to NumPy arrays\n","#         X_test = np.array(X_test)\n","#         y_test = np.array(y_test)\n","\n","#         # Flatten the input if it's not already flattened\n","#         # if len(X_test.shape) > 2:\n","#         #     X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","#         # Fit to validation data\n","#         optimizer = Adam(learning_rate=learning_rates[best_lr])\n","#         loss = CategoricalCrossEntropyLoss()\n","\n","#         # Forward pass through the trained model to get predictions\n","#         predictions = model.forward(X_test, training=False)\n","\n","#         # Find the predicted class for each sample (assuming the last layer is softmax)\n","#         predicted_labels = np.argmax(predictions, axis=1)\n","\n","#         # Output the predictions\n","#         # print(predicted_labels)\n","\n","#         # If you need to compute the accuracy against ground truth\n","#         accuracy = np.mean(predicted_labels == y_test)\n","#         print(f\"Test Accuracy: {accuracy * 100:.3f}%\")\n","\n","#         # Find f1 macro\n","#         from sklearn.metrics import f1_score\n","#         f1 = f1_score(y_test, predicted_labels, average='macro')\n","#         print(f\"F1 Score: {f1}\")\n","        \n","#         output_dir = f'report/results/{best_idx}/{best_lr}'\n","#         os.makedirs(output_dir, exist_ok=True)\n","#         with open(os.path.join(output_dir, \"test_acc_scores.log\"), 'w') as file:\n","#             file.write(f\"{accuracy}\")\n","            \n","#         with open(os.path.join(output_dir, \"test_f1_scores.log\"), 'w') as file:\n","#             file.write(f\"{f1}\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"myenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
